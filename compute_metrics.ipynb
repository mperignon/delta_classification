{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "import cPickle as pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "# import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "# from numpy.random import rand\n",
    "\n",
    "# from scipy.io import loadmat\n",
    "from scipy.ndimage import morphology\n",
    "# from scipy.cluster.vq import kmeans,vq\n",
    "from scipy import signal, ndimage\n",
    "\n",
    "\n",
    "# from make_map import make_map\n",
    "# from file_util import load_matlab_data\n",
    "# from plot import plot_discharge\n",
    "from metrics import *\n",
    "# from stratigraphy import sedimentograph\n",
    "from spatial_dist import *\n",
    "\n",
    "\n",
    "from shapely.geometry import shape, Point, Polygon, MultiLineString, MultiPoint, MultiPolygon, LineString\n",
    "from shapely.ops import polygonize_full #, transform\n",
    "\n",
    "import rasterio\n",
    "# from rasterio.features import shapes\n",
    "from osgeo import ogr, gdal, osr\n",
    "import fiona\n",
    "\n",
    "import clusterpy\n",
    "import skimage\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from metrics_scripts.metrics_utils import *\n",
    "\n",
    "\n",
    "def SaveRaster(array, filename, r_xmin, r_ymin, res, downsampling = 1):\n",
    "    '''\n",
    "    Saves a numpy array into a raster file\n",
    "    '''\n",
    "\n",
    "    rasterOrigin = (r_xmin, r_ymin)\n",
    "    pixelWidth = res * downsampling\n",
    "    pixelHeight = res * downsampling\n",
    "\n",
    "    reversed_arr = array[::-1] # reverse array so the tif looks like the array\n",
    "    array2raster(filename,rasterOrigin,pixelWidth,pixelHeight,reversed_arr) # convert array to raster\n",
    "    \n",
    "    \n",
    "\n",
    "def array2raster(newRasterfn, rasterOrigin, pixelWidth, pixelHeight, array, spatial_ref = 32645):\n",
    "    '''\n",
    "    Converts numpy array into raster, given a model raster file\n",
    "    '''\n",
    "\n",
    "    cols = array.shape[1]\n",
    "    rows = array.shape[0]\n",
    "    originX = rasterOrigin[0]\n",
    "    originY = rasterOrigin[1]\n",
    "    \n",
    "    output_raster = gdal.GetDriverByName('GTiff').Create(newRasterfn, cols, rows, 1, gdal.GDT_Float32)\n",
    "    output_raster.GetRasterBand(1).WriteArray( array ) \n",
    "    output_raster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(spatial_ref)\n",
    "    output_raster.SetProjection(srs.ExportToWkt())\n",
    "    output_raster.FlushCache()\n",
    "\n",
    "    \n",
    "    \n",
    "def latlong_to_index(pt, xmin, xmax, ymin, ymax, array_size):\n",
    "    '''\n",
    "    Calculates index of point on array from latitude and longitude\n",
    "    '''\n",
    "    \n",
    "    newx = int(array_size[1] * (pt[0] - xmin) / (xmax - xmin))\n",
    "    newy = int(array_size[0] - array_size[0] * (pt[1] - ymin) / (ymax - ymin))\n",
    "    \n",
    "    return (newx, newy)\n",
    "\n",
    "\n",
    "\n",
    "def create_shapefile_from_shapely_multi(features, filename,\n",
    "                                        fields = {}, field_type = {},\n",
    "                                        buffer_width = 0, spatial_ref = 32645):\n",
    "    '''\n",
    "    Creates a shapefile from a\n",
    "    Shapely MultiPolygon, MultiLineString, or MultiPoint\n",
    "    '''\n",
    "\n",
    "\n",
    "    driver = ogr.GetDriverByName('Esri Shapefile')\n",
    "    ds = driver.CreateDataSource(filename)\n",
    "\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(spatial_ref)\n",
    "\n",
    "    layer = ds.CreateLayer('', srs, ogr.wkbPolygon)\n",
    "\n",
    "    for f in fields.keys():\n",
    "        fieldDefn = ogr.FieldDefn(f, field_type[f])\n",
    "        layer.CreateField(fieldDefn)\n",
    "\n",
    "    defn = layer.GetLayerDefn()\n",
    "\n",
    "\n",
    "    for i in range(len(features)):\n",
    "\n",
    "        poly = features[i].buffer(buffer_width)\n",
    "\n",
    "        # Create a new feature (attribute and geometry)\n",
    "        feat = ogr.Feature(defn)\n",
    "\n",
    "        for f in fields.keys():\n",
    "            feat.SetField(f, fields[f][i])\n",
    "\n",
    "        # Make a geometry from Shapely object\n",
    "        geom = ogr.CreateGeometryFromWkb(poly.wkb)\n",
    "        feat.SetGeometry(geom)\n",
    "\n",
    "        layer.CreateFeature(feat)\n",
    "        feat = geom = None  # destroy these\n",
    "\n",
    "\n",
    "    # Save and close everything\n",
    "    ds = layer = feat = geom = None\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def create_tiff_from_shapefile(InputVector, OutputImage, RefImage):\n",
    "    '''\n",
    "    Rasterizes a shapefile\n",
    "    '''\n",
    "\n",
    "    gdalformat = 'GTiff'\n",
    "    datatype = gdal.GDT_Byte\n",
    "    burnVal = 1 #value for the output image pixels\n",
    "\n",
    "    ##########################################################\n",
    "    # Get projection info from reference image\n",
    "    Image = gdal.Open(RefImage, gdal.GA_ReadOnly)\n",
    "\n",
    "    # Open Shapefile\n",
    "    Shapefile = ogr.Open(InputVector)\n",
    "    Shapefile_layer = Shapefile.GetLayer()\n",
    "\n",
    "    # Rasterise\n",
    "    Output = gdal.GetDriverByName(gdalformat).Create(OutputImage,\n",
    "                                                     Image.RasterXSize,\n",
    "                                                     Image.RasterYSize,\n",
    "                                                     1,\n",
    "                                                     datatype,\n",
    "                                                     options=['COMPRESS=DEFLATE'])\n",
    "    Output.SetProjection(Image.GetProjectionRef())\n",
    "    Output.SetGeoTransform(Image.GetGeoTransform()) \n",
    "\n",
    "    # Write data to band 1\n",
    "    Band = Output.GetRasterBand(1)\n",
    "    Band.SetNoDataValue(0)\n",
    "    gdal.RasterizeLayer(Output, [1], Shapefile_layer, burn_values=[burnVal])\n",
    "\n",
    "    # Close datasets\n",
    "    Band = None\n",
    "    Output = None\n",
    "    Image = None\n",
    "    Shapefile = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_of_raster_within_polygons(raster, polygons, r_xmin, r_xmax, r_ymin, r_ymax, min_valid_label = None):\n",
    "    '''\n",
    "    Samples raster values at representative point of polygons\n",
    "    '''\n",
    "    \n",
    "    poly_points_utm = [i.representative_point().coords[0] for i in polygons]\n",
    "    poly_points_index = [latlong_to_index(i, r_xmin, r_xmax, r_ymin, r_ymax, raster.shape) for i in poly_points_utm]\n",
    "    \n",
    "    poly_points_value = [raster[i[::-1]] for i in poly_points_index]\n",
    "    \n",
    "    step = 10\n",
    "\n",
    "    while (min_valid_label is not None) & (step < 25):\n",
    "\n",
    "        bad_pts = np.where(np.array(poly_points_value) < min_valid_label)[0]\n",
    "\n",
    "        if len(bad_pts) > 0:\n",
    "            \n",
    "            for i in bad_pts:\n",
    "\n",
    "                pts = RegularGridSampling(polygons[i], step = step)\n",
    "                pts_utm = [p.coords[0] for p in pts]\n",
    "                pts_index = [latlong_to_index(p, r_xmin, r_xmax, r_ymin, r_ymax, raster.shape) for p in pts_utm]\n",
    "                pts_value = [raster[p[::-1]] for p in pts_index]\n",
    "                good_pts = np.where(np.array(pts_value) >= min_valid_label)[0]\n",
    "\n",
    "                if len(good_pts) > 0:\n",
    "                    poly_points_value[i] = pts_value[good_pts[0]]\n",
    "                    poly_points_index[i] = pts_index[good_pts[0]]\n",
    "                    \n",
    "            step += 5\n",
    "        else:\n",
    "            step = 100\n",
    "            \n",
    "    bad_pts = np.where(np.array(poly_points_value) < min_valid_label)[0]\n",
    "    \n",
    "    return poly_points_value, poly_points_index, bad_pts\n",
    "    \n",
    "\n",
    "def RegularGridSampling(polygon, x_interval = None, y_interval = None, step = None):\n",
    "    \"\"\"\n",
    "    Perform sampling by substituting the polygon with a regular grid of\n",
    "    sample points within it. The distance between the sample points is\n",
    "    given by x_interval and y_interval.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    if step is None:\n",
    "        step = 10\n",
    "    \n",
    "    if x_interval is None:\n",
    "        x_interval = (polygon.bounds[2] - polygon.bounds[0]) / step\n",
    "        \n",
    "    if y_interval is None:   \n",
    "        y_interval = (polygon.bounds[3] - polygon.bounds[1]) / step\n",
    "    \n",
    "    ll = polygon.bounds[:2]\n",
    "    ur = polygon.bounds[2:]\n",
    "    low_x = int(ll[0]) / x_interval * x_interval\n",
    "    upp_x = int(ur[0]) / x_interval * x_interval + x_interval\n",
    "    low_y = int(ll[1]) / y_interval * y_interval\n",
    "    upp_y = int(ur[1]) / y_interval * y_interval + y_interval\n",
    "\n",
    "    for x in np.arange(low_x, upp_x, x_interval):\n",
    "        for y in np.arange(low_y, upp_y, y_interval):\n",
    "            p = Point(x, y)\n",
    "            if p.within(polygon):\n",
    "                samples.append(p)\n",
    "                \n",
    "    \n",
    "                \n",
    "    return MultiPoint(samples)\n",
    "\n",
    "def azimuth(point1, point2):\n",
    "    '''azimuth between 2 shapely points (interval 0 - 360), from vertical'''\n",
    "    \n",
    "    angle = np.arctan2(point2.x - point1.x, point2.y - point1.y)\n",
    "    az = np.degrees(angle) if angle > 0 else np.degrees(angle) + 360\n",
    "    \n",
    "    return az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics for distributions\n",
    "\n",
    "def mean_(val, freq):\n",
    "    return np.average(val, weights = freq)\n",
    "\n",
    "def median_(val, freq):\n",
    "    order = np.argsort(val)\n",
    "    cdf = np.cumsum(freq[order])\n",
    "    return val[order][np.searchsorted(cdf, cdf[-1] // 2)]\n",
    "\n",
    "def mode_(val, freq): #in the strictest sense, assuming unique mode\n",
    "    return val[np.argmax(freq)]\n",
    "\n",
    "def var_(val, freq):\n",
    "    avg = mean_(val, freq)\n",
    "    dev = freq * (val - avg) ** 2\n",
    "    return dev.sum() / (freq.sum() - 1)\n",
    "\n",
    "def std_(val, freq):\n",
    "    return np.sqrt(var_(val, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Polygon_axes(polygon):\n",
    "    '''\n",
    "    Calculates major and minor axes, and major axis azimuth, for a Shapely polygon\n",
    "    '''\n",
    "\n",
    "    rect = polygon.minimum_rotated_rectangle.exterior\n",
    "\n",
    "    len1 = LineString(rect.coords[0:2]).length\n",
    "    len2 = LineString(rect.coords[1:3]).length\n",
    "\n",
    "    if len1 > len2:\n",
    "        orientation = azimuth(Point(rect.coords[0]), Point(rect.coords[1]))\n",
    "    else:\n",
    "        orientation = azimuth(Point(rect.coords[1]), Point(rect.coords[2]))\n",
    "        \n",
    "    if orientation > 180:\n",
    "        orientation = orientation - 180\n",
    "\n",
    "    axminor, axmajor = np.sort([len1, len2])\n",
    "\n",
    "    return axminor, axmajor, orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CDF(data):\n",
    "    '''\n",
    "    Calculate the Cumulative Distribution Function (CDF) of a 1D array of data\n",
    "    '''\n",
    "\n",
    "    data_sorted = np.flipud(np.sort(data))\n",
    "    p = 1. * np.arange(len(data)) / (len(data) - 1)\n",
    "\n",
    "    return [data_sorted, p]\n",
    "\n",
    "\n",
    "\n",
    "# data_sorted, p = get_CDF(island_prop_array[:,24])\n",
    "\n",
    "# plt.figure()\n",
    "# plt.loglog(data_sorted, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline_to_mask(line, x, y):\n",
    "    \"\"\"\n",
    "    Create mask from outline contour\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    line: array-like (N, 2)\n",
    "    x, y: 1-D grid coordinates (input for meshgrid)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : 2-D boolean array (True inside)\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from shapely.geometry import Point\n",
    "    >>> poly = Point(0,0).buffer(1)\n",
    "    >>> x = np.linspace(-5,5,100)\n",
    "    >>> y = np.linspace(-5,5,100)\n",
    "    >>> mask = outline_to_mask(poly.boundary, x, y)\n",
    "    \n",
    "    Modified from https://gist.github.com/perrette/a78f99b76aed54b6babf3597e0b331f8\n",
    "    \n",
    "    \"\"\"\n",
    "    import matplotlib.path as mplp\n",
    "    mpath = mplp.Path(line)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    points = np.array((X.flatten(), Y.flatten())).T\n",
    "    mask = mpath.contains_points(points).reshape(X.shape)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _grid_bbox(x, y):\n",
    "    dx = dy = 0\n",
    "    return x[0]-dx/2, x[-1]+dx/2, y[0]-dy/2, y[-1]+dy/2\n",
    "\n",
    "def _bbox_to_rect(bbox):\n",
    "    l, r, b, t = bbox\n",
    "    return Polygon([(l, b), (r, b), (r, t), (l, t)])\n",
    "\n",
    "def shp_mask(shp, x, y, m=None):\n",
    "    \"\"\"\n",
    "    Use recursive sub-division of space and shapely contains method to create a raster mask on a regular grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shp : shapely's Polygon (or whatever with a \"contains\" method and intersects method)\n",
    "    x, y : 1-D numpy arrays defining a regular grid\n",
    "    m : mask to fill, optional (will be created otherwise)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    m : boolean 2-D array, True inside shape.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from shapely.geometry import Point\n",
    "    >>> poly = Point(0,0).buffer(1)\n",
    "    >>> x = np.linspace(-5,5,100)\n",
    "    >>> y = np.linspace(-5,5,100)\n",
    "    >>> mask = shp_mask(poly, x, y)\n",
    "    \"\"\"\n",
    "    rect = _bbox_to_rect(_grid_bbox(x, y))\n",
    "    \n",
    "    if m is None:\n",
    "        m = np.zeros((y.size, x.size), dtype=bool)\n",
    "               \n",
    "    if not shp.intersects(rect):\n",
    "        m[:] = False\n",
    "    \n",
    "    elif shp.contains(rect):\n",
    "        m[:] = True\n",
    "    \n",
    "    else:\n",
    "        k, l = m.shape\n",
    "        \n",
    "        if k == 1 and l == 1:\n",
    "            m[:] = shp.contains(Point(x[0], y[0]))\n",
    "            \n",
    "        elif k == 1:\n",
    "            m[:, :l//2] = shp_mask(shp, x[:l//2], y, m[:, :l//2])\n",
    "            m[:, l//2:] = shp_mask(shp, x[l//2:], y, m[:, l//2:])\n",
    "            \n",
    "        elif l == 1:\n",
    "            m[:k//2] = shp_mask(shp, x, y[:k//2], m[:k//2])\n",
    "            m[k//2:] = shp_mask(shp, x, y[k//2:], m[k//2:])\n",
    "        \n",
    "        else:\n",
    "            m[:k//2, :l//2] = shp_mask(shp, x[:l//2], y[:k//2], m[:k//2, :l//2])\n",
    "            m[:k//2, l//2:] = shp_mask(shp, x[l//2:], y[:k//2], m[:k//2, l//2:])\n",
    "            m[k//2:, :l//2] = shp_mask(shp, x[:l//2], y[k//2:], m[k//2:, :l//2])\n",
    "            m[k//2:, l//2:] = shp_mask(shp, x[l//2:], y[k//2:], m[k//2:, l//2:])\n",
    "        \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def island_properties(islands, smooth = True):\n",
    "    '''\n",
    "    Identifies islands and calculates island morphological properties\n",
    "    \n",
    "    Input:\n",
    "    -------\n",
    "    islands: raster of land (1) vs water\n",
    "    smooth: boolean for median filter to simplify island mask\n",
    "    \n",
    "    Output:\n",
    "    --------\n",
    "    A list containing:\n",
    "    island_IMG: array of islands by label\n",
    "    \n",
    "    If properties = True, also contains:\n",
    "    island_props: dictionary of island properties, by label\n",
    "    ecdf: cumulative probability distributions of island size\n",
    "    edgedist_IMG: array of islands with calculated distances from edges\n",
    "    edgedist_hist: histogram of edge distances\n",
    "    \n",
    "    '''\n",
    "\n",
    "    tot_area = islands.sum()\n",
    "\n",
    "    if smooth: \n",
    "        islands_filt = signal.medfilt2d(islands.astype(float), 3)\n",
    "    else:\n",
    "        islands_filt = islands.astype(float)\n",
    "\n",
    "    islandmap, N = ndimage.label(islands_filt)\n",
    "    islandmap = islandmap.astype('int')\n",
    "    \n",
    "    island_props = {}\n",
    "    \n",
    "    # for island properties\n",
    "    rps = skimage.measure.regionprops(islandmap, cache=False)\n",
    "\n",
    "    Li = [r.major_axis_length for r in rps if r.minor_axis_length > 0]\n",
    "    Wi = [r.minor_axis_length for r in rps if r.minor_axis_length > 0]\n",
    "    Pi = [r.perimeter for r in rps if r.minor_axis_length > 0]\n",
    "    Ai = [r.area for r in rps if r.minor_axis_length > 0]\n",
    "    label = [r.label for r in rps if r.minor_axis_length > 0]\n",
    "\n",
    "    island_area = [float(a) / tot_area for a in Ai]\n",
    "    island_aspect_ratio = [Li[n] / Wi[n] for n in range(len(Li))]\n",
    "    island_shape_factor = [Pi[n] / np.sqrt(Ai[n]) for n in range(len(Li))]\n",
    "    \n",
    "    \n",
    "    bad_labels = [int(i) for i in np.unique(islandmap) if i not in label]\n",
    "\n",
    "    for i in bad_labels:\n",
    "        \n",
    "        islandmap[islandmap == i] = 0\n",
    "\n",
    "    \n",
    "    island_props['major_axis'] = Li\n",
    "    island_props['minor_axis'] = Wi\n",
    "    island_props['perimeter'] = Pi\n",
    "    island_props['area'] = island_area\n",
    "    island_props['aspt_ratio'] = island_aspect_ratio\n",
    "    island_props['shp_factor'] = island_shape_factor\n",
    "    island_props['label'] = label\n",
    "    \n",
    "    return islandmap, island_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_flag = False\n",
    "\n",
    "raster_filepath = '_input/gangeschan.tif'\n",
    "network_filepath = '_input/network.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster, (r_xmin, r_xmax, r_ymin, r_ymax, r_xres, r_yres) = read_tiff_as_array(raster_filepath, get_info= True)\n",
    "islandmap = (raster > 1) * 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get islands from network shapefile\n",
    "\n",
    "c = fiona.open(network_filepath)\n",
    "\n",
    "network_lines = MultiLineString([shape(pol['geometry']) for pol in c])\n",
    "network_widths = [pol['properties']['Width'] for pol in c]\n",
    "network_azimuths = [azimuth(Point(l.coords[0]),Point(l.coords[-1])) for l in network_lines]\n",
    "\n",
    "result, dangles, cuts, invalids = polygonize_full(network_lines)\n",
    "islands = MultiPolygon([i for i in result if i.area > r_xres * r_xres * 10])\n",
    "\n",
    "c = None\n",
    "\n",
    "\n",
    "# find islands with interior polygons\n",
    "# first number is index of island with interior polygons\n",
    "# second number is index of interior island\n",
    "\n",
    "get_contained_islands = preprocess_flag\n",
    "\n",
    "if get_contained_islands:\n",
    "    \n",
    "    contained_islands = []\n",
    "\n",
    "    for i in range(len(islands)):\n",
    "\n",
    "        # if island has an inner ring\n",
    "        if not islands[i].exterior.equals(islands[i].boundary):\n",
    "\n",
    "            # check all other islands\n",
    "            for j in range(len(islands)):\n",
    "                if i != j:\n",
    "\n",
    "                    # check if one is inside the other\n",
    "                    inside = islands[j].within(Polygon(islands[i].exterior))\n",
    "\n",
    "                    if inside:\n",
    "                        contained_islands.append([i,j])\n",
    "\n",
    "    pickle.dump(contained_islands, open( '_output/contained_islands' + '.p', \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    contained_islands = pickle.load( open( '_output/contained_islands.p', \"rb\" ) )\n",
    "\n",
    "\n",
    "bad_islands = set([i[1] for i in contained_islands])\n",
    "holey_islands = set([i[0] for i in contained_islands])\n",
    "\n",
    "new_islands = [Polygon(islands[i].exterior) if i in holey_islands else islands[i] for i in range(len(islands))]\n",
    "new_islands = [new_islands[i] for i in range(len(new_islands)) if i not in bad_islands]\n",
    "\n",
    "islands = MultiPolygon(new_islands)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bounding_channels = preprocess_flag\n",
    "\n",
    "if get_bounding_channels:\n",
    "\n",
    "    # midpoints of network lines, with buffers\n",
    "    midpts = [l.interpolate(0.5, normalized=True).buffer(5) for l in network_lines]\n",
    "\n",
    "    bounds = []\n",
    "    interior_channels = []\n",
    "\n",
    "    # check if line midpoints intersect the island outlines\n",
    "    #(to identify which lines make up each island)\n",
    "    for polygon in islands:\n",
    "    \n",
    "        touch = [i for i,l in enumerate(midpts) if polygon.exterior.intersects(l)]\n",
    "        bounds.append(touch)\n",
    "        \n",
    "        touch = [i for i,l in enumerate(midpts) if polygon.contains(l)]\n",
    "        interior_channels.append(touch)\n",
    "\n",
    "    pickle.dump(bounds, open( '_output/island_boundary_channels' + '.p', \"wb\" ) )\n",
    "    pickle.dump(interior_channels, open( '_output/island_interior_channels' + '.p', \"wb\" ) )\n",
    "\n",
    "else:    \n",
    "    bounds = pickle.load( open( '_output/island_boundary_channels.p', \"rb\" ) )\n",
    "    interior_channels = pickle.load( open( '_output/island_interior_channels.p', \"rb\" ) )\n",
    "    \n",
    "flat_bounds = np.unique([item for sublist in bounds for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create shapefile and raster of the network islands\n",
    "\n",
    "create_island_GIS = preprocess_flag\n",
    "\n",
    "if create_island_GIS:\n",
    "\n",
    "    channel_bounds = [network_lines[i] for i in flat_bounds]\n",
    "\n",
    "    create_shapefile_from_shapely_multi(MultiLineString(channel_bounds),\n",
    "                                        '_output/islands_polygons.shp',\n",
    "                                        buffer_width = r_xres/2.)\n",
    "\n",
    "\n",
    "    create_tiff_from_shapefile('_output/islands_polygons.shp',\n",
    "                               '_output/islands_polygons.tif',\n",
    "                               '_input/gangeschan.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etch the network islands onto the islandmap found from the image\n",
    "# to highlight channels that are not seen as continuous in the image\n",
    "#\n",
    "# Then run island_properties again and get island properties\n",
    "\n",
    "get_new_island_properties = preprocess_flag\n",
    "pickle_island_properties = preprocess_flag\n",
    "\n",
    "if get_new_island_properties:\n",
    "\n",
    "    islandmap1, island_props0 = island_properties(islandmap, smooth = False)\n",
    "\n",
    "    # remove invalid labels (set with min_valid_label) and\n",
    "    # then enough of the smallest island labels to have\n",
    "    # the same number as islands from the network\n",
    "\n",
    "    min_valid_label = 3\n",
    "\n",
    "    num_id_islands = len(island_props0['label'])\n",
    "    expected_num_islands = len(islands)\n",
    "\n",
    "    diff_number = num_id_islands - expected_num_islands - min_valid_label\n",
    "\n",
    "\n",
    "    # label must be less than min_valid_label\n",
    "    # then remove the smallest diff_number islands\n",
    "\n",
    "    area_sort = np.argsort(island_props0['area'])\n",
    "\n",
    "    flag = False\n",
    "    dn = 0\n",
    "\n",
    "    while (not flag) & (dn <= min_valid_label):\n",
    "\n",
    "        dn += 1\n",
    "        small_island_id = [i for i in area_sort[:diff_number+dn] if island_props0['label'][i] >= min_valid_label]\n",
    "        \n",
    "        flag = len(small_island_id) == diff_number + dn\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    keep_props_id = [i for i in area_sort[diff_number+dn:] if island_props0['label'][i] >= min_valid_label]\n",
    "\n",
    "    island_props = {}\n",
    "\n",
    "    for f,v in island_props0.items():\n",
    "        island_props[f] = np.array(island_props0[f])[keep_props_id]\n",
    "        \n",
    "    bad_labels = [int(i) for i in np.unique(islandmap1) if i not in island_props['label']]\n",
    "\n",
    "    for i in bad_labels:\n",
    "        islandmap1[islandmap1 == i] = 0\n",
    "        \n",
    "        \n",
    "    if pickle_island_properties:\n",
    "    \n",
    "        pickle.dump(island_props, open( '_output/island_props' + '.p', \"wb\" ) )\n",
    "        SaveRaster(islandmap1, '_output/island_map_HD.tif', r_xmin, r_ymin, r_xres)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    island_props = pickle.load( open( '_output/island_props.p', \"rb\" ) )\n",
    "    islandmap1 = read_tiff_as_array('_output/island_map_HD.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_channel_props = preprocess_flag\n",
    "\n",
    "if get_channel_props:\n",
    "\n",
    "    # associate channel properties to islands\n",
    "\n",
    "    network_min_widths = np.zeros((len(islands),))\n",
    "    network_avg_widths = np.zeros((len(islands),))\n",
    "    network_max_widths = np.zeros((len(islands),))\n",
    "\n",
    "    for n in range(len(islands)):\n",
    "\n",
    "        i = islands[n]\n",
    "\n",
    "        # channels added in open water to close island polygons have width 9999\n",
    "        channels = [network_lines[b] for b in bounds[n] if network_widths[b] != 9999]\n",
    "        widths = [network_widths[b] for b in bounds[n] if network_widths[b] != 9999]\n",
    "        lengths = [c.length for c in channels]\n",
    "\n",
    "\n",
    "        tot_length = sum(lengths)\n",
    "        avg_width = sum([widths[b] * lengths[b] for b in range(len(widths))]) / tot_length\n",
    "        max_width = max(widths)\n",
    "        min_width = min(widths)\n",
    "\n",
    "\n",
    "        network_min_widths[n] = min_width\n",
    "        network_avg_widths[n] = avg_width\n",
    "        network_max_widths[n] = max_width\n",
    "        \n",
    "    pickle.dump([network_min_widths, network_avg_widths, network_max_widths],\n",
    "                open( '_output/channel_widths' + '.p', \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    network_min_widths, network_avg_widths, network_max_widths = pickle.load(\n",
    "                            open( '_output/channel_widths.p', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_poly_label = preprocess_flag\n",
    "\n",
    "# poly_label is an array of the size of islands with the label from islandmap1\n",
    "#(and therefore the connection to island_props)\n",
    "\n",
    "if get_poly_label:\n",
    "\n",
    "    poly_label, poly_index, bad_pts = get_value_of_raster_within_polygons(islandmap1.astype('int'),\n",
    "                                                                          islands,\n",
    "                                                                          r_xmin, r_xmax,\n",
    "                                                                          r_ymin, r_ymax,\n",
    "                                                                          min_valid_label=3)\n",
    "    poly_label = np.array(poly_label)\n",
    "\n",
    "    pickle.dump([poly_label, poly_index], open( '_output/poly_label' + '.p', \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    poly_label, poly_index = pickle.load( open( '_output/poly_label.p', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_poly_metrics = preprocess_flag\n",
    "\n",
    "if get_poly_metrics:\n",
    "\n",
    "    num_ox = []\n",
    "\n",
    "    for i in islands:\n",
    "\n",
    "        oxs = 0\n",
    "        for j in i.interiors:\n",
    "\n",
    "            try:\n",
    "                a = Polygon(j).buffer(-20).area\n",
    "\n",
    "                if a > 40000:\n",
    "                    oxs += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        num_ox.append(oxs)\n",
    "\n",
    "\n",
    "\n",
    "    interior_lengths = [sum([network_lines[j].length for j in interior_channels[i]]) if len(interior_channels[i])>0 else 0 for i in range(len(shp_islands))] \n",
    "\n",
    "    perimeter = np.array([i.boundary.length for i in shp_islands])\n",
    "    wetted_perimeter = perimeter + 2 * np.array(interior_lengths)   \n",
    "    area = np.array([i.area for i in shp_islands])\n",
    "    perimeter_convex_hull = np.array([i.convex_hull.exterior.length for i in shp_islands])\n",
    "    area_convex_hull = np.array([i.convex_hull.area for i in shp_islands])\n",
    "\n",
    "    a = np.array(map(Polygon_axes, shp_islands))\n",
    "    minor_axis = a[:,0]\n",
    "    major_axis = a[:,1]\n",
    "    poly_orientation = a[:,2]\n",
    "    aspect_ratio = major_axis / minor_axis\n",
    "\n",
    "    circularity = 4 * np.pi * area / perimeter**2\n",
    "    equivalent_area_diameter = np.sqrt((4 / np.pi) * area)\n",
    "    perimeter_equivalent_diameter = area / np.pi\n",
    "    solidity = area / area_convex_hull\n",
    "    concavity = area_convex_hull - area\n",
    "    convexity = perimeter_convex_hull / perimeter\n",
    "    dry_shape_factor = perimeter / np.sqrt(area)\n",
    "    wet_shape_factor = wetted_perimeter / np.sqrt(area)\n",
    "\n",
    "    poly_metrics = {'p_area': area,\n",
    "                    'p_perim': perimeter,\n",
    "                    'p_wetperim': wetted_perimeter,\n",
    "                    'p_ch_area': area_convex_hull,\n",
    "                    'p_ch_perim': perimeter_convex_hull,\n",
    "#                     'p_max_rad': maximum_inscribed_circle_radius,\n",
    "                    'p_major_ax': major_axis,\n",
    "                    'p_minor_ax': minor_axis,\n",
    "                    'p_asp_rat': aspect_ratio,\n",
    "                    'p_orient': poly_orientation,\n",
    "                    'p_circ': circularity,\n",
    "                    'p_eq_a_dia': equivalent_area_diameter,\n",
    "                    'p_p_eq_dia': perimeter_equivalent_diameter,\n",
    "                    'p_solidity': solidity,\n",
    "                    'p_concav': concavity,\n",
    "                    'p_convex': convexity,\n",
    "                    'p_d_shapef': dry_shape_factor,\n",
    "                    'p_w_shapef': wet_shape_factor,\n",
    "                    'p_label': shp_islands_label,\n",
    "                    'p_num_ox': num_ox}\n",
    "\n",
    "    pickle.dump(poly_metrics, open( '_output/island_shp_metrics' + '.p', \"wb\" ) )\n",
    "\n",
    "else:\n",
    "\n",
    "    poly_metrics = pickle.load( open( '_output/island_shp_metrics.p', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_islands = islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_sinuosity = preprocess_flag\n",
    "\n",
    "\n",
    "if calc_sinuosity:\n",
    "\n",
    "    sinuosity_all = np.ones((len(shp_islands), 3))\n",
    "    \n",
    "    for cn, c_dist in enumerate([500, 1000, 1500]):\n",
    "\n",
    "    \n",
    "        for ni,i in enumerate(shp_islands):\n",
    "            sinuosity = np.ones((len(bounds[ni]),))\n",
    "\n",
    "        \n",
    "            for n,b in enumerate(bounds[ni]):\n",
    "                line = network_lines[b]\n",
    "\n",
    "                \n",
    "                if line.length > c_dist:\n",
    "\n",
    "                    l_dist = []\n",
    "\n",
    "                    for s in range(0, c_dist, 100):\n",
    "\n",
    "                        coords = []\n",
    "\n",
    "                        for d in np.arange(s, line.length+1, c_dist):  \n",
    "                            coords.append(line.interpolate(d).coords[0])\n",
    "\n",
    "                        diff = np.diff(np.array(coords), axis=0)\n",
    "                        l_dist += list(np.sqrt(diff[:,0]**2 + diff[:,1]**2))\n",
    "\n",
    "\n",
    "                        \n",
    "                    rline = LineString(list(line.coords)[::-1])    \n",
    "                    for s in range(0, c_dist, 100):\n",
    "\n",
    "                        coords = []\n",
    "\n",
    "                        for d in np.arange(s, rline.length+1, c_dist):  \n",
    "                            coords.append(rline.interpolate(d).coords[0])\n",
    "\n",
    "                        diff = np.diff(np.array(coords), axis=0)\n",
    "                        l_dist += list(np.sqrt(diff[:,0]**2 + diff[:,1]**2))\n",
    "\n",
    "                        \n",
    "                        \n",
    "                    sin = np.median(c_dist / np.array(l_dist))\n",
    "                    sinuosity[n] = sin\n",
    "\n",
    "\n",
    "            sinuosity_all[ni,cn] = np.mean(sinuosity)\n",
    "\n",
    "\n",
    "    pickle.dump(sinuosity_all, open( '_output/bound_channel_sinuosity_vals' + '.p', \"wb\" ))\n",
    "\n",
    "    \n",
    "else:\n",
    "    \n",
    "    sinuosity_all = pickle.load( open( '_output/bound_channel_sinuosity_vals.p', \"rb\" ) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find angle that outflow channel makes with boundary\n",
    "\n",
    "find_outflow_angles = preprocess_flag\n",
    "\n",
    "if find_outflow_angles:\n",
    "\n",
    "    num_outflow = np.zeros((len(islands),))\n",
    "\n",
    "    for n in range(len(islands)):\n",
    "\n",
    "        lines = [i for i in interior_channels[n]]\n",
    "\n",
    "        # interior channels that touch the boundary\n",
    "        outflow = []\n",
    "\n",
    "        for l in lines:\n",
    "            if islands[n].exterior.touches(network_lines[l]):\n",
    "                outflow.append(l)\n",
    "\n",
    "        angle = []\n",
    "        num_out = 0\n",
    "\n",
    "        for l in outflow:\n",
    "\n",
    "            inside_line = shp_islands[n].intersection(network_lines[l])\n",
    "            node = shp_islands[n].boundary.intersection(network_lines[l])\n",
    "\n",
    "            if node.type is 'GeometryCollection':\n",
    "                node = shp_islands[n].boundary.intersection(network_lines[l])\n",
    "\n",
    "            if inside_line.type is 'LineString':\n",
    "                num_out += 1\n",
    "\n",
    "        \n",
    "        num_outflow[n] = num_out\n",
    "\n",
    "    pickle.dump(num_outflow, open( '_output/outflow_number' + '.p', \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    num_outflow = pickle.load( open( '_output/outflow_number.p', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bound_angles = preprocess_flag\n",
    "\n",
    "if get_bound_angles:\n",
    "\n",
    "    all_channel_angles = []\n",
    "\n",
    "    for isl in range(len(islands)):\n",
    "\n",
    "        lines = [network_lines[l] for l in bounds[isl]]\n",
    "        num_lines = [l for l in bounds[isl]]\n",
    "\n",
    "        channel_angles = []\n",
    "\n",
    "\n",
    "        for l1,l2 in itertools.combinations(range(len(lines)),2):\n",
    "\n",
    "            if lines[l1].touches(lines[l2]):\n",
    "\n",
    "                node_ = lines[l1].intersection(lines[l2])\n",
    "\n",
    "                if node_.type is 'Point':\n",
    "                    node_ = [node_]\n",
    "\n",
    "                for node in node_:\n",
    "\n",
    "                    third_line = [n for n,i in enumerate(network_lines) if (i.intersects(node.buffer(20)) and\n",
    "                                                                            n not in [num_lines[l1],num_lines[l2]])]\n",
    "\n",
    "                    if len(third_line) > 0:\n",
    "\n",
    "                        intersecting_lines = third_line + [num_lines[l1], num_lines[l2]]\n",
    "\n",
    "\n",
    "                        line0 = network_lines[intersecting_lines[0]]\n",
    "                        line1 = network_lines[intersecting_lines[1]]\n",
    "                        line2 = network_lines[intersecting_lines[2]]\n",
    "\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            buffer_l = 200\n",
    "\n",
    "                            intersect0 = line0.intersection(node.buffer(buffer_l).exterior)\n",
    "                            if intersect0.type is 'MultiPoint':\n",
    "                                intersect0 = intersect0[0]\n",
    "                            az0 = azimuth(node, intersect0)\n",
    "\n",
    "                            intersect1 = line1.intersection(node.buffer(buffer_l).exterior)\n",
    "                            if intersect1.type is 'MultiPoint':\n",
    "                                intersect1 = intersect1[0]\n",
    "                            az1 = azimuth(node, intersect1)\n",
    "\n",
    "                            intersect2 = line2.intersection(node.buffer(buffer_l).exterior)\n",
    "                            if intersect2.type is 'MultiPoint':\n",
    "                                intersect2 = intersect2[0]\n",
    "                            az2 = azimuth(node, intersect2)\n",
    "\n",
    "                        except:\n",
    "\n",
    "                            buffer_l = np.min([line0.simplify(10000).length * 0.8,\n",
    "                            line1.simplify(1000).length * 0.8,\n",
    "                            line2.simplify(1000).length * 0.8,\n",
    "                            200])\n",
    "\n",
    "                            intersect0 = line0.intersection(node.buffer(buffer_l).exterior)\n",
    "                            if intersect0.type is 'MultiPoint':\n",
    "                                intersect0 = intersect0[0]\n",
    "                            az0 = azimuth(node, intersect0)\n",
    "\n",
    "                            intersect1 = line1.intersection(node.buffer(buffer_l).exterior)\n",
    "                            if intersect1.type is 'MultiPoint':\n",
    "                                intersect1 = intersect1[0]\n",
    "                            az1 = azimuth(node, intersect1)\n",
    "\n",
    "                            intersect2 = line2.intersection(node.buffer(buffer_l).exterior)\n",
    "                            if intersect2.type is 'MultiPoint':\n",
    "                                intersect2 = intersect2[0]\n",
    "                            az2 = azimuth(node, intersect2)\n",
    "\n",
    "                        channel_angles.append([az0, az1, az2])\n",
    "\n",
    "        all_channel_angles.append(channel_angles)\n",
    "\n",
    "    angle_stats = np.zeros((len(all_channel_angles), 5))\n",
    "\n",
    "    for n, a in enumerate(all_channel_angles):\n",
    "\n",
    "        if len(a) > 0:\n",
    "\n",
    "            angle_differences_ = np.zeros((len(a), 3))\n",
    "\n",
    "            for nn, r in enumerate(a):\n",
    "\n",
    "                az0, az1, az2 = r\n",
    "\n",
    "                az01 = np.abs(az0 - az1)\n",
    "                az01 = az01 if az01 < 180 else az01 - 180\n",
    "                az02 = np.abs(az0 - az2)\n",
    "                az02 = az02 if az02 < 180 else az02 - 180\n",
    "                az12 = 360 - az01 - az02\n",
    "\n",
    "\n",
    "                angle_differences_[nn,:] = [az01,az02,az12]\n",
    "\n",
    "\n",
    "            diff = angle_differences_.flatten()\n",
    "            stats = [np.min(diff), np.max(diff), np.mean(diff), np.median(diff), np.std(diff)]\n",
    "\n",
    "            angle_stats[n,:] = stats\n",
    "        \n",
    "    pickle.dump(angle_stats, open( '_output/bound_channel_angle_stats' + '.p', \"wb\" ) )\n",
    "    pickle.dump(all_channel_angles, open( '_output/bound_channel_all_angles' + '.p', \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    angle_stats = pickle.load( open( '_output/bound_channel_angle_stats.p', \"rb\" ) )\n",
    "    all_channel_angles = pickle.load( open( '_output/bound_channel_all_angles.p', \"rb\" ) )\n",
    "\n",
    "angle_differences = []\n",
    "angle_stats = np.zeros((len(all_channel_angles), 5))\n",
    "\n",
    "for n, a in enumerate(all_channel_angles):\n",
    "\n",
    "    if len(a) > 0:\n",
    "\n",
    "        angle_differences_ = np.zeros((len(a), 3))\n",
    "\n",
    "        for nn, r in enumerate(a):\n",
    "\n",
    "            az0, az1, az2 = r\n",
    "\n",
    "            az01 = np.abs(az0 - az1)\n",
    "            az01 = az01 if az01 < 180 else az01 - 180\n",
    "            az02 = np.abs(az0 - az2)\n",
    "            az02 = az02 if az02 < 180 else az02 - 180\n",
    "            az12 = 360 - az01 - az02\n",
    "\n",
    "\n",
    "            angle_differences_[nn,:] = [az01,az02,az12]\n",
    "\n",
    "        angle_differences.append(angle_differences_)\n",
    "\n",
    "        diff = angle_differences_.flatten()\n",
    "        stats = [np.min(diff), np.max(diff), np.mean(diff), np.median(diff), np.std(diff)]\n",
    "\n",
    "        angle_stats[n,:] = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_stats2 = np.zeros((len(all_channel_angles), 3))\n",
    "\n",
    "for n, a in enumerate(all_channel_angles):\n",
    "\n",
    "    if len(a) > 0:\n",
    "\n",
    "        sort = [i.sort() for i in a]\n",
    "\n",
    "        small_angle = [i[0] for i in a]\n",
    "        mid_angle = [i[1] for i in a]\n",
    "        large_angle = [i[2] for i in a]\n",
    "\n",
    "        angle_stats2[n,:] = [np.mean(small_angle), np.mean(mid_angle), np.mean(large_angle)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_edge_dist = preprocess_flag\n",
    "\n",
    "if calc_edge_dist:\n",
    "\n",
    "    edgedists = np.zeros((len(shp_islands),))\n",
    "\n",
    "    for n,s in enumerate(shp_islands):\n",
    "\n",
    "        print n\n",
    "\n",
    "        cellsize = 10\n",
    "\n",
    "        minx, miny, maxx, maxy = s.envelope.bounds\n",
    "\n",
    "        if ((maxx - minx) / cellsize > 1000) or ((maxy - miny) / cellsize > 1000):\n",
    "            cellsize = 100\n",
    "\n",
    "\n",
    "\n",
    "        minx = np.floor(minx) - 1 * cellsize\n",
    "        maxx = np.ceil(maxx) + 2 * cellsize\n",
    "        miny = np.floor(miny) - 1 * cellsize\n",
    "        maxy = np.ceil(maxy) + 2 * cellsize\n",
    "\n",
    "        x = np.arange(minx, maxx , cellsize)\n",
    "        y = np.arange(miny, maxy , cellsize)\n",
    "\n",
    "\n",
    "        mask = outline_to_mask(s.exterior, x, y)\n",
    "        distmap = morphology.distance_transform_edt(mask)\n",
    "\n",
    "        edgedists[n] = distmap.max() * cellsize\n",
    "\n",
    "\n",
    "    pickle.dump(edgedists, open( '_output/edge_distances' + '.p', \"wb\" ) )\n",
    "\n",
    "else:\n",
    "    \n",
    "    edgedists = pickle.load( open( '_output/edge_distances.p', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "# save island_props to a shapefile\n",
    "\n",
    "new_props = {'min_width':network_min_widths,\n",
    "    'avg_width':network_avg_widths,\n",
    "             'edge_d2': edgedists,\n",
    "             'max_width':network_max_widths,\n",
    "#              'o_ang_min': outflow_stats[:,0],\n",
    "#              'o_ang_max': outflow_stats[:,1],\n",
    "#              'o_ang_mean': outflow_stats[:,2],\n",
    "#              'o_ang_med': outflow_stats[:,3],\n",
    "#              'o_ang_std': outflow_stats[:,4],\n",
    "             'out_numbr': num_outflow,\n",
    "#              'ch_ang_min': angle_stats[:,0],\n",
    "#              'ch_ang_max': angle_stats[:,1],\n",
    "#              'ch_ang_med': angle_stats[:,3],\n",
    "#              'ch_ang_std': angle_stats[:,4],\n",
    "             'sin500': sinuosity_all[:,0],\n",
    "            'sin1000': sinuosity_all[:,1],\n",
    "            'sin1500': sinuosity_all[:,2],\n",
    "#              'ang0': angle_stats2[:,0],\n",
    "#             'ang1': angle_stats2[:,1],\n",
    "#             'ang2': angle_stats2[:,2],\n",
    "            }\n",
    "\n",
    "all_keys = new_props.keys() + poly_metrics.keys()\n",
    "\n",
    "\n",
    "\n",
    "island_prop_array = np.zeros((len(islands),len(all_keys)+1))\n",
    "\n",
    "for i in island_props['label']:\n",
    "    \n",
    "    loc = np.where(np.array(island_props['label']) == i)[0]\n",
    "    loc_poly = np.where(poly_label == i)[0]\n",
    "    \n",
    "    if (len(loc) > 0) & (len(loc_poly) > 0):\n",
    "        \n",
    "        loc = loc[0]\n",
    "        props1 = []\n",
    "        props2 = [new_props[f][loc_poly[0]] for f in new_props.keys()]\n",
    "        props3 = [poly_metrics[f][loc_poly[0]] for f in poly_metrics.keys()]\n",
    "        \n",
    "        \n",
    "        for p in loc_poly:\n",
    "            \n",
    "            island_prop_array[p,:-1] = props1 + props2 + props3\n",
    "\n",
    "island_prop_array[np.isnan(island_prop_array)] = 0\n",
    "\n",
    "island_prop_array[:,-1] = range(0,len(islands))\n",
    "all_keys.append('id')\n",
    "\n",
    "\n",
    "pickle.dump(island_prop_array, open( '_output/island_prop_array' + '.p', \"wb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "field_type = {}\n",
    "\n",
    "for k in all_keys:\n",
    "    field_type[k] = ogr.OFTReal\n",
    "    \n",
    "    \n",
    "bad_islands = np.where(island_prop_array[:,0] == 0)[0]\n",
    "\n",
    "island_prop_array = island_prop_array[island_prop_array[:,0] > 0,:] # keep only the lines that have a label\n",
    "shp_islands2 = [islands[i] for i in range(len(islands)) if i not in bad_islands]\n",
    "    \n",
    "\n",
    "fields = {}    \n",
    "for n,f in enumerate(all_keys):\n",
    "    fields[f] = island_prop_array[:,n]\n",
    "    \n",
    "\n",
    "create_shapefile_from_shapely_multi(shp_islands2,\n",
    "                                    '_output/islands_properties.shp',\n",
    "                                    fields = fields,\n",
    "                                    field_type = field_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
